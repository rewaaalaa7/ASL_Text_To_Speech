ASL (American Sign Language) Text-to-Speech 

This project aims to bridge the communication gap between the hearing-impaired and the hearing community by developing a system that translates American Sign Language (ASL) gestures into spoken language. The project utilizes Convolutional Neural Networks (CNNs) for classifying ASL hand gestures captured through image data.

Objective:
The primary objective of this project is to develop a robust ASL recognition system that accurately interprets hand gestures and translates them into corresponding text or speech.

Dataset: 
https://www.kaggle.com/datasets/ayuraj/asl-dataset/data

Integration with Text-to-Speech (TTS) System: 
Upon successful classification of ASL gestures, the project integrates a Text-to-Speech (TTS) system to convert the recognized gestures into spoken language.
